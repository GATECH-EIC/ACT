# Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green)](https://opensource.org/licenses/Apache-2.0)

Zhongzhi Yu<sup>1,*</sup>, Zheng Wang<sup>1,*</sup>, Yonggan Fu<sup>1</sup>, Huihong Shi<sup>1</sup>, Khalid Shaikh<sup>1</sup>, Yingyan (Celine) Lin<sup>1</sup>

*<sup>1</sup> Georgia Institute of Technology*

Accepted by **[ICML 2024](https://icml.cc/Conferences/2024)**

More info: [[Paper](https://openreview.net/pdf?id=DLTjFFiuUJ)]

The code is coming soon, stay tuned! 
